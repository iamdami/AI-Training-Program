21-12-30
===
- Resnet50  
-ImageNet(ILSVRC) 2015 대회에서 우승한 모델  
 50개 레이어 있는 딥러닝 신경망 모델  
 이미지 분류, 이미지에서의 객체 감지 위한 백본 모델로 자주 사용  
 반복적인 곱셈은 그라디언트를 극도로 작게 만들고 모델 학습 중지하게 함  
 -> 입력이 활성화 계층으로 건너뛰기해 도달하는 연결로 그라디언트 소실 문제 해결  
 [resnet50](https://www.annytab.com/resnet50-image-classification-in-python/)  
  
  
- 그라디언트 소실  
-심층 인공 신경망 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient) 점차적으로 작아지는 현상 발생할 수 있음  
입력층에 가까운 층들에서 가중치 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 됨  
이를 기울기 소실(Gradient Vanishing) 이라고 함  
[그라디언트 소실](https://wikidocs.net/61375)  
  
**-기울기 소실을 완화하는 가장 간단한 방법: 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것**  
  
은닉층에서는 시그모이드 함수를 사용하지 말 것  
Leaky ReLU 사용하면 모든 입력값에 대해 기울기가 0에 수렴하지 않아 죽은 ReLU 문제 해결  
은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형 사용할 것!!  

